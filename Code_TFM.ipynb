{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicción del daño sísmico en Gorkha, 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En Abril de 2015, Gorkha, ciudad de 32.000 habitantes que se encuentra en Nepal, fue víctima de un terremoto de magnitud 7.8. Este desastre natural produjo unas pérdidas de 10 billones de dólares, lo cual representa la mitad del PIB de Nepal, por lo que el gobierno tuvo que entrar en largos y costosos procesos de reparación.\n",
    "\n",
    "Dentro de este proceso de reconstrucción y análisis de los daños causados, se tuvo que identificar cuál fue el nivel de destrucción de las infraestructras. Y es en este punto donde juega un papel importante el análisis de datos, y concretamente el trabajo de un Data Scientist, ya que trataremos de analizar el Dataset Ritcher’s Predictor: Modeling Earthquake Damage. En este caso, el dataset estudiado cuenta con los datos de las infraestructuras dañadas de Gorkha y  todas sus características que más tarde pasaremos a analizar detalladamente. De esta manera, para determinar qué nivel de reparación necesita una infraestructura dañada, es necesario catalogar el nivel de destrucción de esta después de la catástrofe. Una vez hecho esto, es posible confeccionar un modelo capaz de predecir cual sería el nivel de destrucción de una casa, a partir de sus características, en caso de que ocurriese una catástrofe del mismo calibre.\n",
    "\n",
    "Para ello, pasamos a estudiar nuestro dataset Ritcher's Predictor en el que analizaremos las variables incluídas y la naturaleza del propio conjunto de nuestros datos. Después, trataremos de solucionar los problemas que puedan tener las variables de nuestro conjunto de datos y finalemente lo prepararemos para realizar la mejor predicción posible del nivel de daño causado a un edificio por el terremoto de Gorkha.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, HTML\n",
    "Image(url='https://s3.amazonaws.com/drivendata-public-assets/nepal-quake-bm-2.JPG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, accuracy_score, precision_score\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import make_pipeline as imb_make_pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')\n",
    "pd.set_option(\"display.max_columns\",None)\n",
    "pd.set_option(\"display.max_rows\",None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducción y presentación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ya hemos comentado anteriormente, nuestro cometido en este trabajo es el de preparar un modelo que nos haga predecir de la forma más precisa la categoría de daño que puede sufrir una casa a partir de un terremoto como el de Gorkha en 2015. Podemos realizar esta operación ya que tenemos dos tipos de 'datasets' con los que trabajar; Por un lado, trabajaremos con nuestro conjunto de entrenamiento que llamaremos 'data', en el que nos indican las caracteríticas de cada edificio y su categoría de daño, y después tendremos nuestro conjunto 'test' que será el que utilizaremos para predecir cuál es el nivel de daño causado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los dos conjuntos de datos y la variable respuesta.\n",
    "\n",
    "data = pd.read_csv('train_values.csv')\n",
    "test = pd.read_csv('test_values.csv')\n",
    "train_labels = pd.read_csv('train_labels.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En nuestro conjunto de datos de entrenamiento tenemos 39 features, contando con nuestra variable de identificación 'building_id'. Juntamos nuestro dataframe con nuestra variable objetivo 'damage_grade' para poder trabajar en un mismo dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo unimos en nuestro dataset de entrenamiento\n",
    "\n",
    "data = data.merge(train_labels, how = 'inner' , on = 'building_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasamos a comprobar que nuestra base de datos no disponga de valores nulos o duplicados. Realizamos la comprobación con una tabla inormativa que contenga el tipo de datos de cada variable, la cantidad de valores nulos que incluye y cuantos valores unicos tienen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla informatriva acerca del tipo (Int,object)\n",
    "\n",
    "tipo_datos=pd.DataFrame(data.dtypes,columns=[\"Type\"])\n",
    "\n",
    "tipo_datos[\"Unique\"]=data.nunique()\n",
    "tipo_datos[\"Null\"]=data.isnull().sum()\n",
    "tipo_datos[\"% null\"]=data.isnull().sum()/len(data)\n",
    "\n",
    "tipo_datos.style.background_gradient(cmap='Set3',axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como nos enseña la tabla que hemos construído, no obtenemos valores faltantes ni duplicados en 'building_id'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analisis variables cuantitativas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos estudiar primero de todo a nuestra variable objetivo 'damage_grade', así que con un histograma podemos ver que categorías son las que más se dan. Después nos ocuparemos del resto de variables cuantitativas y observaremos si tienen problemas tales como valores outliers, mala codificación,etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representamos histograma de nuestra variable objetivo 'damage_grade'\n",
    "\n",
    "plt.figure(figsize = (11,7))\n",
    "ax = sns.countplot('damage_grade',data = data)\n",
    "\n",
    "for i in ax.patches :\n",
    "    ax.text(i.get_x() + 0.3, i.get_height() + 3, \\\n",
    "            str(round((i.get_height()),2)), fontsize = 13 )\n",
    "plt.title('Damage_grade')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como muestra el histograma del código anterior, la categoria de dañado medio es la que más datos tiene en nuestro conjunto de entrenamiento, por lo que con esta información podemos ser conscientes del comportamiento de otras variables. Pasamos ahora a observar las variables numéricas de nuestro dataset, concretamente las de localización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos listas de variables cuantitativas que tengan algo en común. Juntamos las de localización por una parte y las de condiciones físicas del edificio por otra.\n",
    "\n",
    "gps_features = ['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id']\n",
    "\n",
    "area_height = ['area_percentage' , 'height_percentage']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para fijarnos en la distribucón de las variables de localización que las hemos incluido en una lista llamada 'gps_features', nos será de gran utilidad observar la dispersión que muestran las tres variables por cada grupo de nuestra variable respuesta 'damage_grade'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot de las tres variables de localización (geo_level_1_id, geo_level_2_id, geo_level_3_id)\n",
    "\n",
    "plt.figure(figsize = (18,18))\n",
    "q = 1\n",
    "\n",
    "for i in gps_features:\n",
    "    plt.subplot(3,3,q)\n",
    "    ax = sns.boxplot('damage_grade', i, data = data)\n",
    "    plt.xlabel('damage_grade')\n",
    "    plt.ylabel(i)\n",
    "    plt.title('Damage_grade per region' + str(i))\n",
    "    q += 1\n",
    "    plt.show\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nos fijamos en nuestros diagramas de cajas para nuestras tres variables de localización de las infraestructuras dañadas, lo primero que observamos es que en geo_level_3 tenemos una variabilidad entre los tres grupos de daño mucho mas baja, ya que en esta variable la localización es más concreta.\n",
    "En lo que se refiere a los sectores 1-30, vemos que la mayor dispersión se encuentra en el grupo de daño menor, ya que tenemos muchas infraetsructuras dañadas levemente. Este efecto puede ser consecuencia de que el terremoto en Gorkha dañó levemente a sitios muy distanciados, por lo que es difícil que esta variable pueda precisar de forma correcta este grupo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representamos gráficamente la distribución de height_percentage y area_percentage, así podemos ver si el comportamiento de esta variable es muy diferente en cada grupo de la variable objetivo. De esta forma, podremos saber si el area o la altura de las infraestructuras dañadas son una característica que afecte al dañado provocado por el terremoto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representación distirbución area_percentage.\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "sns.FacetGrid(data,hue='damage_grade',height=5)\\\n",
    "    .map(sns.distplot,'area_percentage')\\\n",
    "    .add_legend()\n",
    "\n",
    "plt.title(\"Area Percentage\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representación distribución 'height_percentage'.\n",
    "\n",
    "plt.figure(figsize=(14,8))\n",
    "sns.FacetGrid(data,hue='damage_grade',height=5)\\\n",
    "    .map(sns.distplot,'height_percentage')\\\n",
    "    .add_legend()\n",
    "\n",
    "plt.title(\"Height Percentage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque no podamos extarer conclusiones de momento, podemos observar que tanto la altura del edificio como su area pueden influir perfectamente al nivel de destrucción del edificio después del terremoto. Podemos realizarnos la misma pregunta para la variable 'age', ya que tiene bastante sentido pensar que si el edificio es más antiguo resistió menos el efecto del terremoto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representación de 'edad' histograma.\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.countplot(data['age'], hue=data[\"damage_grade\"])\n",
    "\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.title(\"Age by damaged grade\")\n",
    "plt.legend([\"Low\",\"Medium\",\"High\"])\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que la gran mayoría de infraestructuras dañadas tienen alrededor de 5 a 20 años, la gran mayoría de ellas clasificadas con el nivel medio de destrucción. También se muestra que la categoría de daño más bajo se reduce exponencialmente a medida que los edificios son más antiguos, por lo que estos ya reciben un daño medio o destrucción total. Al mismo tiempo, la evolución en términos proporcionales de la categoría 'High' va ganando terreno a 'Medium' a medida que aumenta la edad. Finalmente, eliminaremos más adelante los valores atípicos que encontramos en la variable 'age', ya que se encuentran totalmente alejados de nuestra distribución de datos y pueden no ser representativos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograma de 'count_floors_pre_eq'\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.countplot(data['count_floors_pre_eq'], hue=data[\"damage_grade\"])\n",
    "\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"count_floors_pre_eq\")\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.title(\"Count_floors_pre_eq\")\n",
    "plt.legend([\"Low\",\"Medium\",\"High\"])\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de count_floors_pre_eq se muestra que la categoria de daño 'High' se encuentra más presente en términos proporcionales cuando el edificio en cuestion pasa de tener una planta a tres. Es lógico pensar que esta variable se encuentre correlacionada con 'height_percentage' ya que esta última tambien podria tener un impacto positivo en el daño causado al edificio. De esta manera, count_floors_pre_eq nos podría ser de gran ayuda cuando entrenemos un modelo de predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analisis variables categóricas y binarias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hemos analizado las distribuciones de nuestras variables cuantitativas y ver cuales presentan algunos problemas y si nos podrían ser de interés, podemos hacer lo mismo con las variables categóricas representando su histograma y ver como se comportan las categorías de 'damage_grade'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos lista de categóricas \n",
    "\n",
    "categoricas = ['plan_configuration','land_surface_condition','foundation_type','roof_type',\n",
    "              'ground_floor_type', 'other_floor_type','position','legal_ownership_status']\n",
    "\n",
    "# Realizamos un encuadre de histogramas con nuestras variables categóricas mediante seaborn.\n",
    "    \n",
    "q=1\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "for j in categoricas:\n",
    "    plt.subplot(3,3,q)\n",
    "    ax=sns.countplot(data[j].dropna(), hue=data[\"damage_grade\"])\n",
    "    plt.xlabel(j)\n",
    "    plt.legend([\"Low \",\"Medium\",\"High\"])\n",
    "    q+=1\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizamos, de la misma forma que lo hemos hecho con las variables categóricas, las variables binarias que hacen referencia a la superestructura de los edificios y al de uso secundario de las infraestructuras dañadas. Estos gráficos nos dejan algunas ideas que podemos tomar en consideración que se encuentran explicadas en nuestro informe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos las variables binarias que hacen referencia a la superestructura de los edificios.\n",
    "\n",
    "superstructure_cols = ['has_superstructure_adobe_mud' , 'has_superstructure_mud_mortar_stone' , 'has_superstructure_stone_flag' ,\n",
    "                      'has_superstructure_cement_mortar_stone', 'has_superstructure_mud_mortar_brick' , 'has_superstructure_cement_mortar_brick'\n",
    "                      , 'has_superstructure_timber' , 'has_superstructure_bamboo' , 'has_superstructure_rc_non_engineered'\n",
    "                      , 'has_superstructure_rc_engineered' , 'has_superstructure_other']    \n",
    "\n",
    "q=1\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "# Por cada variable en la lista, generar un histograma e incluirlo a nuestro subplot.\n",
    "\n",
    "for j in superstructure_cols:\n",
    "    plt.subplot(4,4,q)\n",
    "    ax=sns.countplot(data[j].dropna(), hue=data[\"damage_grade\"])\n",
    "    plt.xlabel(j)\n",
    "    plt.legend([\"Low \",\"Medium\",\"High\"])\n",
    "    q+=1\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos la misma operación para las variables binarias de uso secundario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos las variables binarias restantes para observar los histogramas.\n",
    "\n",
    "secondary_use = ['has_secondary_use', 'has_secondary_use_agriculture' , 'has_secondary_use_hotel' , 'has_secondary_use_rental', \n",
    "                    'has_secondary_use_institution' , 'has_secondary_use_school' , 'has_secondary_use_industry', \n",
    "                    'has_secondary_use_health_post','has_secondary_use_gov_office' , 'has_secondary_use_use_police' , \n",
    "                    'has_secondary_use_other' ]\n",
    "\n",
    "q=1\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "# Recorremos la lista para ir incluyendo los histogramas en el subplot generado.\n",
    "\n",
    "for j in secondary_use:\n",
    "    plt.subplot(4,4,q)\n",
    "    ax=sns.countplot(data[j].dropna(), hue=data[\"damage_grade\"])\n",
    "    plt.xlabel(j)\n",
    "    plt.legend([\"Low \",\"Medium\",\"High\"])\n",
    "    q+=1\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocesamiento de datos y valores outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de haber representado y comentado de forma más extensa en el informe las propiedades de las variables independientes y objetivo, es necesario volver a las variables númericas para solucionar el problema de valores outliers que habíamos detectado en variables como por ejemplo 'age'. Para ello, realizamos el proceso winsorized para poder ajustar la distribución de las observaciones extremas. Este proceso se realiza para las variables 'age', 'height_percentage' y 'area_percentage', de manera que ya estarían listas para poder analizar las correlaciones con otras variables (analisis multivariante) y para incluirlas en un modelo de predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outilers en variables numéricas.\n",
    "\n",
    "num_cols=[\"age\",\"area_percentage\",\"height_percentage\"]\n",
    "q=1\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "for j in num_cols:\n",
    "    plt.subplot(3,3,q)\n",
    "    ax=sns.boxplot(data[j].dropna(), palette=\"colorblind\")\n",
    "    plt.xlabel(j)\n",
    "    q+=1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disponemos de valores outliers en las variables que hemos incluído en los gráficos de caja. Deberemos arreglar el problema de los outliers de manera que el efecto de estas variables sobre nuestra variable respuesta no se desvirtualize debido a los valores extremos que observamos. Mostramos una representación del diagrama de cajas de estas tres variables numéricas antes y después del proceso 'winsorized' para comparar como hemos arreglado la dispersión de la variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos la variable 'age' para realizar la estabilicación de los valores outliers que encontramos.\n",
    "\n",
    "n = 'age'\n",
    "target = data[n]\n",
    "\n",
    "# Dibujamos un boxplot para comparar \"winsonorization process\" de nuestros valores outliers.\n",
    "\n",
    "sns.boxplot(target)\n",
    "plt.title(\"{} No Winsorization\".format(n))\n",
    "plt.show()\n",
    "\n",
    "# Realizamos winsonorization y comparamos con el gráfico anterior.\n",
    "\n",
    "winsorized_target = winsorize(target,(0,0.05))\n",
    "\n",
    "# Boxplot winsorized\n",
    "\n",
    "sns.boxplot(winsorized_target)\n",
    "plt.title(\"{} Winsorized\". format(n))\n",
    "plt.show\n",
    "\n",
    "# Las pasamos a nuestro dataset de entrenamiento.\n",
    "\n",
    "data['age'] = winsorized_target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que tenemos una distribución de nuestra variable edad mucho más balanceada al transformar los valores outliers que teníamos anteriormente. En este momento, la centralidad de la variable edad se encuentra entre los 10 y 30 años, con una media de 21 años, antes la teníamos alrededor de los 26 años de edad. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos la variable 'area_percentege' para realizar la estabilicación de los valores outliers que encontramos.\n",
    "\n",
    "n = 'area_percentage'\n",
    "target = data[n]\n",
    "\n",
    "# Dibujamos un boxplot para comparar \"winsonorization process\" de nuestros valores outliers.\n",
    "\n",
    "sns.boxplot(target)\n",
    "plt.title(\"{} No Winsorization\".format(n))\n",
    "plt.show()\n",
    "\n",
    "# Realizamos winsonorization y comparamos con el gráfico anterior.\n",
    "\n",
    "winsorized_target = winsorize(target,(0,0.04))\n",
    "\n",
    "# Boxplot winsorized\n",
    "\n",
    "sns.boxplot(winsorized_target)\n",
    "plt.title(\"{} Winsorized\". format(n))\n",
    "plt.show\n",
    "\n",
    "# Las pasamos a nuestro dataset de entrenamiento\n",
    "\n",
    "data['area_percentage'] = winsorized_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos la variable 'height_percentage' para realizar la estabilicación de los valores outliers que encontramos.\n",
    "\n",
    "n = 'height_percentage'\n",
    "target = data[n]\n",
    "\n",
    "# Dibujamos un boxplot para comparar \"winsonorization process\" de nuestros valores outliers.\n",
    "\n",
    "sns.boxplot(target)\n",
    "plt.title(\"{} No Winsorization\".format(n))\n",
    "plt.show()\n",
    "\n",
    "# Realizamos winsonorization y comparamos con el gráfico anterior.\n",
    "\n",
    "winsorized_target = winsorize(target,(0,0.04))\n",
    "\n",
    "# Boxplot winsorized\n",
    "\n",
    "sns.boxplot(winsorized_target)\n",
    "plt.title(\"{} Winsorized\". format(n))\n",
    "plt.show\n",
    "\n",
    "# Las pasamos a nuestro dataset de entrenamiento\n",
    "\n",
    "data['height_percentage'] = winsorized_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasamos a guardar las features que queremos incluir en nuestro modelo en un nuevo dataframe, tambien realizaremos un gráfico de correlaciones entre las variables incluidas para ver si podemos prescindir de algunas más en nuestro modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analisis correlaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando ya hemos preprocesado las variables que presentaban tener algun problema que pudiera distorsionar la relación que puediesen tener con otras variables o con la propia variable respuesta, estamos listos para crear una matriz de correlaciones y ver de esta manera cuales son las variables que parecen tener relaciones y cuales pueden ser determinantes sobre la variable objetivo 'damage_grade'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nuevo dataset donde tenemos simplemente las variables independientes de nuestro conjunto de datos original.\n",
    "\n",
    "features = data.copy()\n",
    "features = features.drop(['building_id'], axis = 1)\n",
    "\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos una matriz de correlaciones con las features que tenemos incluídas.\n",
    "\n",
    "corr_matrix = features.corr()\n",
    "\n",
    "# Diseñamos un mapa de calor para representar visualmente la matriz de correlaciones.\n",
    "\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "f, ax = plt.subplots(figsize = (14,14))\n",
    "\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "sns.heatmap(corr_matrix, mask=mask, cmap=cmap, vmax=1, vmin = -1, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir del mapa de calor generado en el código anterior podemos ver cuales son las variables que se encuentran más correlacionadas, ya sea de positiva o negativamente.\n",
    "\n",
    "Las variables que más correlacionadas se encuentran con el tipo de daño causado son las que hacen referencia a la superestructura de los edificios,concretamente,'has_superstrcuture_cement_mortar_brick','has_superstructure_rc_engineered', 'has_superstructure_rc_non_engineered' y 'has_superstructure_mud_mortar_stone'. También hemos comentado anteriormente que la edad es una variable que parece tener un efecto causal con el daño causado en el edificio. Después encontramos variables con una gran correlacion como 'count_floors_pre_eq' con 'height_percentage', o 'has_secondary_use' con 'has_secondary_use_agriculture'.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos las feautures que tienen una mayor correlación con nuestra variable otuput 'damage_grade'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapa de calor para ver las correlaciones de 'damage_grade'.\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "correlations = features.corrwith(features['damage_grade'])\n",
    "correlations = correlations.sort_values(ascending = False)\n",
    "sns.heatmap(pd.DataFrame(correlations), annot = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo que sabemos del comportamiento de variables y por lo que hemos visto anteriormente en nuestro correlograma, no es muy recomendable incluir variables que esten muy correlacionadas entre si y que al mismo tiempo no tengan un impacto notable en la variable que queremos predecir. Por ello, trataremos de eliminar de nuestro conjunto de features las variables 'has_secondary_use_agriculture' y has_superstructure_bamboo', ya que no nos serán de gran utilidad en nuestra predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos features con mucha correlación con otras que son más importantes.\n",
    "\n",
    "features = features.drop(['has_secondary_use_agriculture', 'has_superstructure_bamboo'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Entrenamiento y evaluación "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llegamos a la parte más importante del trabajo. Con nuestro dataset preparado para entrenar un modelo, nos quedamos solo con las features que nos permitan realizar la predicción de la forma más precisa posible. Eliminamos 'damage_grade' al ser la variable respuesta, pasamos a tipo 'category' las variables categóricas y después realizamos la partición de los datos para medir la fiabilidad del modelo que eligamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos el dataframe X que emula a nuestro dataset original, de manera que nos servirá para realizar la predicción.\n",
    "X = features\n",
    "\n",
    "X = X.drop(['damage_grade'], axis = 1)\n",
    "\n",
    "# Guardamos nuestra variable output en una lista.\n",
    "y = features['damage_grade']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tenemos nuestro conjunto de datos X proveniente de nuestro training set, pasamos a realizar la partición de nuestro dataset para testear la precisión de nuestra predicción. Finalmente, entrenaremos algunos modelos que nos sean candidatos para realizar la predicción final de nuestro conjunto de testeo, los iremos evaluando en función de la f1_score que obtengan en cada entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test,y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con nuestros dos conjuntos de datos listos para la estimación, empezamos realizando un Random Forest Classifier después de buscar los hiperparámetros claves para la predicción. Despúes observemos la matriz de confusión de nuestro modelo y estudiaremos los resultados en base a la medida f1_score que es en la que se basa la competición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "param_grid = {'n_estimators': [100,150,200,250,300,350,400],'max_depth': [None,10,20,25,30,35,40,45,50]}\n",
    "\n",
    "grid_search_rf = RandomizedSearchCV(rf, param_distributions=param_grid, n_iter=15, \n",
    "                                    cv=5, n_jobs=6, verbose=1)\n",
    "\n",
    "grid_search_rf.fit(X_train,y_train)\n",
    "\n",
    "print(pd.DataFrame(grid_search_rf.cv_results_).sort_values(by='rank_test_score').head())\n",
    "grid_search_rf.score(X_test,y_test)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizamos el modelo Random Forest con los hiperparámteros obtenidos\n",
    "\n",
    "rf = RandomForestClassifier(max_features = None,\n",
    "                            max_depth = 30,\n",
    "                            n_estimators= 150,\n",
    "                            min_samples_split = 3,\n",
    "                            min_samples_leaf = 30,\n",
    "                            random_state = 42)\n",
    "\n",
    "# Lo aplicamos a nuestro conjuntop train y realizamos la predicción sobre el conjunto test.\n",
    "\n",
    "rf.fit(X_train,y_train)\n",
    "rf_pred=rf.predict(X_test)\n",
    "\n",
    "# Diseñamos una matriz de confusión para ver donde ha cometido más errores la predicción del modelo.\n",
    "\n",
    "cm=confusion_matrix(y_test,rf_pred)\n",
    "conf_matrix=pd.DataFrame(data=cm,columns=['Predicted:1','Predicted:2','Predicted:3'],\n",
    "                                         index=['Actual:1','Actual:2','Actual:3'])\n",
    "              \n",
    "# Representamos la matriz gráficamente.\n",
    "    \n",
    "plt.figure(figsize = (8,5))\n",
    "sns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\")\n",
    "plt.title(\"confusion Matrix for  Random Forest\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=360)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos la precisión del modelo y classification report.\n",
    "\n",
    "print('-'*100)\n",
    "print('Accuracy on Random Forest', accuracy_score(y_test,rf_pred))\n",
    "print('-'*100)\n",
    "print(\"\\n\")\n",
    "print(\"Classification Report :\\n\\n \" , classification_report(y_test,rf_pred))\n",
    "print(\"-\"*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De momento, tal y como nos muestra nuestra matriz de confusión, la predicción Random Forest se ajusta bastante bien a los datos observados en y_test. Además obtenemos una accuracy de 0.731, la cual es una buena puntuación comparada con la media que observamos en la competición DrivenData. Veremos si podemos mejorarla con otras alternativas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos visto que el modelo Random Forest funciona con bastante precisión, pero debemos probar con otros modelos que nos permitan obtener una accuracy_score mayor para poder realizar la estimación final out-of-sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos nuestro modelo de predicción boosting con objetivo softmax al tener tres clases de objetivo\n",
    "\n",
    "xgb = XGBClassifier(objective = 'multi:softmax',\n",
    "                    num_class = 3,\n",
    "                    random_state = 42,\n",
    "                    max_depth = 20,\n",
    "                    n_estimators = 150,                    \n",
    "                    )\n",
    "\n",
    "xgb.fit(X_train,y_train)\n",
    "xgb_pred = xgb.predict(X_test)\n",
    "\n",
    "# Definimos nuestra matriz de confusión y la representamos gráficamente\n",
    "\n",
    "cm = confusion_matrix(y_test,xgb_pred)\n",
    "conf_matrix=pd.DataFrame(data=cm,columns=['Predicted:1','Predicted:2','Predicted:3'],\n",
    "                                         index=['Actual:1','Actual:2','Actual:3'])\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize = (8,5))\n",
    "sns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\")\n",
    "plt.title(\"Confusion Matrix for Boosting Classifier\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=360)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*100)\n",
    "print('Accuracy on XGBoosting', accuracy_score(y_test,xgb_pred))\n",
    "print('-'*100)\n",
    "print(\"\\n\")\n",
    "print(\"Classification Report :\\n\\n \" , classification_report(y_test,xgb_pred))\n",
    "print(\"-\"*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo XGBoosting se adapta bien a nuestros datos pero obtiene una medida de precisión más baja que nuestro primer modelo, por lo que Random Forest sería más útil para realizar una predicción. Sin embargo, vemos que XGBoosting clasifica mejor las clases de daño bajo y alto, por lo que podríamos utilizar este modelo para fusionarlo con nuestro Random Forest. De esta manera, probamos con un modelo final que combine los dos que hemos entrenado anteriormente mediante StackingClassifier. Finalmente utilizamos en este modelo una regresión logística como clasificador final para el modelo que combina RandomForest y XGBoosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensamble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos un modelo de ensamble con los dos anteriores que hemos entrenado.\n",
    "\n",
    "models = [\n",
    "     (\"rf\",rf),(\"xgb\", xgb)\n",
    "     \n",
    "]\n",
    "f_model = StackingClassifier(estimators=models, final_estimator=LogisticRegression(),verbose=1,n_jobs=6)\n",
    "f_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "final_pred = f_model.predict(X_test)\n",
    "\n",
    "# Definimos nuestra matriz de confusión y la representamos gráficamente\n",
    "\n",
    "cm = confusion_matrix(y_test,final_pred)\n",
    "conf_matrix=pd.DataFrame(data=cm,columns=['Predicted:1','Predicted:2','Predicted:3'],\n",
    "                                         index=['Actual:1','Actual:2','Actual:3'])\n",
    "\n",
    "\n",
    "plt.figure(figsize = (8,5))\n",
    "sns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\")\n",
    "plt.title(\"Confusion Matrix for Ensamble model\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=360)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*100)\n",
    "print('Accuracy on Ensamble model', accuracy_score(y_test,final_pred))\n",
    "print('-'*100)\n",
    "print(\"\\n\")\n",
    "print(\"Classification Report :\\n\\n \" , classification_report(y_test,final_pred))\n",
    "print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal y como se muestra en la matriz de clasificación y en la precisión de predicción en nuestro conjunto test, el modelo de ensamble final es el que mejor clasifica la variable objetivo damage_grade. Ya que tenemos un modelo ganador, ahora nos toca realizar la predicción out-of-sample con el conjunto test que cargamos al principio del proyecto y guardar nuestras predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Predicción out-of-sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, vamos a realizar la predicción final de nuestro modelo con nuestro conjunto test y podremos subir a la competición nuestras predicciones con el modelo final. Sin embargo, antes debemos preprocesar el conjunto test de la misma manera que lo hemos hecho con nuestro conjunto de entrenamiento durante todo el trabajo; Esto incluye eliminar las variables que finalmente no se incluyeron, arreglar el problema de los outliers con winsorized y transformar las variables categóricas a binarias para mejorar la predicción. Después de tener el conjunto de datos preparado ya solo nos queda realizar la predicción final de nuestro conjunto test y guardar la predicción con el formato requerido de la competición de DrivenData para que nos pueda calcular nuestra puntuacón de f1_score final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables eliminadas.\n",
    "\n",
    "test = test.drop(['building_id','has_secondary_use_agriculture','has_superstructure_bamboo'], axis = 1)\n",
    "\n",
    "# Outliers.\n",
    "\n",
    "n = 'age'\n",
    "target = test[n] \n",
    "winsorized_age = winsorize(target,(0,0.05))\n",
    "test['age'] = winsorized_age\n",
    "\n",
    "\n",
    "n = 'area_percentage'\n",
    "target = test[n] \n",
    "winsorized_area = winsorize(target,(0,0.04))\n",
    "test['area_percentage'] = winsorized_area\n",
    "\n",
    "\n",
    "n = 'height_percentage'\n",
    "target = test[n] \n",
    "winsorized_height = winsorize(target,(0,0.04))\n",
    "test['height_percentage'] = winsorized_height\n",
    "\n",
    "# Pasamos las variables categóricas a dummies.\n",
    "\n",
    "test = pd.get_dummies(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizamos la predicción final\n",
    "\n",
    "prediction = f_model.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos un nuevo dataframe donde se incluya la identificación de las infraestructuras del conjunto test y el 'damage_grade' que ha predecido nuestro algoritmo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el formato de subida que nos requiere DrivenData\n",
    "\n",
    "my_submission = pd.read_csv('submission_format.csv')\n",
    "my_submission = my_submission.drop('damage_grade', axis = 1)\n",
    "\n",
    "# Subimos nuestra predicción.\n",
    "\n",
    "prediction = pd.DataFrame(prediction)\n",
    "my_submission['damage_grade'] = prediction\n",
    "my_submission = my_submission.set_index('building_id')\n",
    "\n",
    "my_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos nuestra predicción en un csv.\n",
    "\n",
    "my_submission.to_csv('my_submission1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head my_submission1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez generado el dataframe con nuestras predicciones para el conjunto test, lo subimos a la competición de DrivenData con el formato que se requiere. Finalmente, la f1_score obtenida en el conjunto test es de 0.7401, lo cual es una gran puntuación ya que nos situamos en el ranking 447 de los 4361 participantes totales. La mejora de nuestra predicción viene dada por el uso de un modelo más óptimo y por el procesamiento que hemos realizado anteriormente con nuestros datos, para ello ha sido imprescindible entender y analizar el comportamiento de nuestras variables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
